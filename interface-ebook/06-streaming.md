# 6ì¥: ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤

ìŠ¤íŠ¸ë¦¬ë°(Streaming)ì€ LLMì˜ ì‘ë‹µì„ **ì „ì²´ê°€ ì™„ì„±ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ê¸ˆì”© ë°›ì•„ì˜¤ëŠ”** ë°©ì‹ì…ë‹ˆë‹¤. ChatGPT ì›¹ ì¸í„°í˜ì´ìŠ¤ì²˜ëŸ¼ í…ìŠ¤íŠ¸ê°€ íƒ€ì´í•‘ë˜ë“¯ ë‚˜íƒ€ë‚˜ëŠ” íš¨ê³¼ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì™œ ìŠ¤íŠ¸ë¦¬ë°ì´ í•„ìš”í•œê°€?

### ì¼ê´„ ì‘ë‹µ (Non-streaming)

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}]
)

# â³ 10ì´ˆ ëŒ€ê¸°...

print(response.choices[0].message.content)
# ì „ì²´ ì‘ë‹µì´ í•œ ë²ˆì— ì¶œë ¥
```

**ë¬¸ì œì **:
- âŒ ì‚¬ìš©ìê°€ ì˜¤ë«ë™ì•ˆ ê¸°ë‹¤ë ¤ì•¼ í•¨
- âŒ ì‘ë‹µì´ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì„
- âŒ UX ì €í•˜

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

# "ì˜›" "ë‚ " " " "ì˜›" "ì " "ì—" " " ...
# ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¨ì–´ê°€ ì¶œë ¥ë¨!
```

**ì¥ì **:
- âœ… ì¦‰ê°ì ì¸ í”¼ë“œë°±
- âœ… ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜
- âœ… ê¸´ ì‘ë‹µì—ë„ ë°˜ì‘ì„± ìœ ì§€
- âœ… ì¤‘ê°„ì— ì·¨ì†Œ ê°€ëŠ¥

## OpenAI ìŠ¤íŠ¸ë¦¬ë°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Pythonì´ ë­ì•¼?"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

### ì²­í¬(Chunk) êµ¬ì¡°

ê° ì²­í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```python
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",  # "chunk"ì„ì— ì£¼ëª©
  "created": 1699564800,
  "model": "gpt-4-0613",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",     # ì²« ì²­í¬ì—ë§Œ
        "content": "Pythonì€"     # ì‹¤ì œ í…ìŠ¤íŠ¸ ì¡°ê°
      },
      "finish_reason": null      # ë§ˆì§€ë§‰ ì²­í¬ì—ì„œë§Œ "stop" ë“±
    }
  ]
}
```

**ì²­í¬ ì‹œí€€ìŠ¤ ì˜ˆì‹œ**:

```python
# Chunk 1
{"delta": {"role": "assistant", "content": ""}, "finish_reason": null}

# Chunk 2
{"delta": {"content": "Python"}, "finish_reason": null}

# Chunk 3
{"delta": {"content": "ì€"}, "finish_reason": null}

# Chunk 4
{"delta": {"content": " í”„ë¡œê·¸ë˜ë°"}, "finish_reason": null}

# ...

# ë§ˆì§€ë§‰ Chunk
{"delta": {}, "finish_reason": "stop"}
```

### ì „ì²´ ì‘ë‹µ ì¬êµ¬ì„±

```python
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ì•ˆë…•"}],
    stream=True
)

full_response = ""
for chunk in stream:
    if chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
        full_response += content
        print(content, end="", flush=True)

print(f"\n\nì „ì²´ ì‘ë‹µ: {full_response}")
```

### finish_reason ì²˜ë¦¬

```python
for chunk in stream:
    choice = chunk.choices[0]

    if choice.delta.content:
        print(choice.delta.content, end="", flush=True)

    if choice.finish_reason == "stop":
        print("\nâœ“ ì •ìƒ ì™„ë£Œ")
    elif choice.finish_reason == "length":
        print("\nâš  max_tokens ë„ë‹¬")
    elif choice.finish_reason == "function_call":
        print("\nğŸ”§ í•¨ìˆ˜ í˜¸ì¶œ ìš”ì²­")
    elif choice.finish_reason == "content_filter":
        print("\nğŸš« ì»¨í…ì¸  í•„í„°ë§")
```

## Function Callingê³¼ ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œë„ í•¨ìˆ˜ í˜¸ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "ë‚ ì”¨ ì •ë³´ ì¡°íšŒ",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }
        }
    }
]

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ì„œìš¸ ë‚ ì”¨ëŠ”?"}],
    tools=tools,
    stream=True
)

function_name = ""
function_args = ""

for chunk in stream:
    delta = chunk.choices[0].delta

    # í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ê°€ ì¡°ê°ë‚˜ì„œ ì˜´
    if delta.tool_calls:
        for tool_call in delta.tool_calls:
            if tool_call.function.name:
                function_name += tool_call.function.name
            if tool_call.function.arguments:
                function_args += tool_call.function.arguments

    if chunk.choices[0].finish_reason == "tool_calls":
        print(f"í•¨ìˆ˜ í˜¸ì¶œ: {function_name}({function_args})")
        # í•¨ìˆ˜ ì‹¤í–‰ ë¡œì§...
```

## Server-Sent Events (SSE)

ìŠ¤íŠ¸ë¦¬ë°ì€ ë‚´ë¶€ì ìœ¼ë¡œ **Server-Sent Events** í”„ë¡œí† ì½œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### SSE í˜•ì‹

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":" world"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### ì§ì ‘ SSE ì²˜ë¦¬ (ê³ ê¸‰)

```python
import requests
import json

response = requests.post(
    "https://api.openai.com/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    },
    json={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith("data: "):
            data = line[6:]  # "data: " ì œê±°
            if data == "[DONE]":
                break
            chunk = json.loads(data)
            if chunk["choices"][0]["delta"].get("content"):
                print(chunk["choices"][0]["delta"]["content"], end="")
```

## Anthropic ìŠ¤íŠ¸ë¦¬ë°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### ì´ë²¤íŠ¸ ê¸°ë°˜ ì²˜ë¦¬

```python
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
) as stream:
    for event in stream:
        if event.type == "content_block_start":
            print("ì‘ë‹µ ì‹œì‘")
        elif event.type == "content_block_delta":
            print(event.delta.text, end="")
        elif event.type == "content_block_stop":
            print("\nì‘ë‹µ ì™„ë£Œ")
        elif event.type == "message_stop":
            print(f"í† í° ì‚¬ìš©: {stream.message.usage}")
```

## ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•©

### FastAPI + SSE

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI
import json

app = FastAPI()
client = OpenAI()

@app.get("/chat")
async def chat_stream(message: str):
    def generate():
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": message}],
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                # SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                data = {
                    "content": chunk.choices[0].delta.content,
                    "done": False
                }
                yield f"data: {json.dumps(data)}\n\n"

        # ì™„ë£Œ ì‹ í˜¸
        yield f"data: {json.dumps({'done': True})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### JavaScript í´ë¼ì´ì–¸íŠ¸

```javascript
// EventSource API ì‚¬ìš©
const eventSource = new EventSource('/chat?message=ì•ˆë…•í•˜ì„¸ìš”');

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.done) {
        eventSource.close();
        console.log('ì‘ë‹µ ì™„ë£Œ');
    } else {
        // í™”ë©´ì— í…ìŠ¤íŠ¸ ì¶”ê°€
        document.getElementById('response').textContent += data.content;
    }
};

eventSource.onerror = (error) => {
    console.error('SSE ì—ëŸ¬:', error);
    eventSource.close();
};
```

### Fetch API + Streaming

```javascript
async function streamChat(message) {
    const response = await fetch('/chat', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({message})
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const {done, value} = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = JSON.parse(line.slice(6));
                if (data.content) {
                    document.getElementById('response').textContent += data.content;
                }
            }
        }
    }
}
```

## React í†µí•© ì˜ˆì œ

```typescript
import { useState } from 'react';
import OpenAI from 'openai';

export function ChatComponent() {
    const [response, setResponse] = useState('');
    const [isStreaming, setIsStreaming] = useState(false);

    const handleSubmit = async (message: string) => {
        setIsStreaming(true);
        setResponse('');

        const client = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY,
            dangerouslyAllowBrowser: true  // í”„ë¡œë•ì…˜ì—ì„œëŠ” ë°±ì—”ë“œ ì‚¬ìš©!
        });

        const stream = await client.chat.completions.create({
            model: 'gpt-4',
            messages: [{role: 'user', content: message}],
            stream: true
        });

        for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            setResponse(prev => prev + content);
        }

        setIsStreaming(false);
    };

    return (
        <div>
            <div className="response">
                {response}
                {isStreaming && <span className="cursor">â–‹</span>}
            </div>
            <button onClick={() => handleSubmit('ì•ˆë…•í•˜ì„¸ìš”')}>
                ì „ì†¡
            </button>
        </div>
    );
}
```

## ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì—°ê²°

### Timeout ì²˜ë¦¬

```python
import signal

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

# íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ)
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(30)

try:
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "ê¸´ ì‘ë‹µ ìš”ì²­"}],
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="")

    signal.alarm(0)  # íƒ€ì„ì•„ì›ƒ í•´ì œ

except TimeoutException:
    print("\nâ± ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
```

### ì¬ì‹œë„ ë¡œì§

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def stream_with_retry(messages):
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        stream=True
    )

    response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            response += content
            yield content

    return response

# ì‚¬ìš©
try:
    for content in stream_with_retry([{"role": "user", "content": "ì•ˆë…•"}]):
        print(content, end="")
except Exception as e:
    print(f"ìµœì¢… ì‹¤íŒ¨: {e}")
```

## ì„±ëŠ¥ ìµœì í™”

### ë²„í¼ë§

```python
import time

class StreamBuffer:
    def __init__(self, flush_interval=0.1):
        self.buffer = []
        self.flush_interval = flush_interval
        self.last_flush = time.time()

    def add(self, text):
        self.buffer.append(text)

        # ì¼ì • ì‹œê°„ë§ˆë‹¤ ë˜ëŠ” ë²„í¼ê°€ ì°¨ë©´ í”ŒëŸ¬ì‹œ
        if time.time() - self.last_flush > self.flush_interval or len(self.buffer) > 10:
            self.flush()

    def flush(self):
        if self.buffer:
            print(''.join(self.buffer), end='', flush=True)
            self.buffer = []
            self.last_flush = time.time()

# ì‚¬ìš©
buffer = StreamBuffer()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì‘ë‹µ"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        buffer.add(chunk.choices[0].delta.content)

buffer.flush()  # ë§ˆì§€ë§‰ ë²„í¼ ë¹„ìš°ê¸°
```

## ì¤‘ê°„ ì·¨ì†Œ

```python
import threading

class CancellableStream:
    def __init__(self):
        self.cancelled = False

    def cancel(self):
        self.cancelled = True

    def stream(self, messages):
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            stream=True
        )

        for chunk in stream:
            if self.cancelled:
                print("\nâ¹ ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œë¨")
                break

            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="")

# ì‚¬ìš©
cs = CancellableStream()

# ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë°
thread = threading.Thread(
    target=cs.stream,
    args=([{"role": "user", "content": "ë§¤ìš° ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}],)
)
thread.start()

# ì‚¬ìš©ìê°€ ì·¨ì†Œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
# cs.cancel()
```

## Codex CLIì—ì„œì˜ ì‹¤ì œ êµ¬í˜„

Codex CLIëŠ” **ëª¨ë“  ìš”ì²­ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬**í•©ë‹ˆë‹¤. ì¼ê´„ ì‘ë‹µ ëª¨ë“œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì„¤ì •

**ìœ„ì¹˜**: `codex-rs/core/src/chat_completions.rs:331-370`

```rust
// í˜ì´ë¡œë“œì— stream: true ì„¤ì •
let payload = json!({
    "model": model_family.slug,
    "messages": messages,
    "stream": true,  // í•­ìƒ ìŠ¤íŠ¸ë¦¬ë°
    "tools": tools_json,
});

// SSE (Server-Sent Events) ìˆ˜ë½
let req_builder = provider
    .create_request_builder(client, &None)
    .await?
    .header(reqwest::header::ACCEPT, "text/event-stream")  // SSE
    .json(&payload);

let response = req_builder.send().await?;
```

**í¬ì¸íŠ¸**:
- `Accept: text/event-stream` í—¤ë”ë¡œ SSE ìš”ì²­
- ëª¨ë“  Chat Completion í˜¸ì¶œì´ ìŠ¤íŠ¸ë¦¬ë°

### ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì•„í‚¤í…ì²˜

**ìœ„ì¹˜**: `codex-rs/core/src/chat_completions.rs:374-388`

```rust
if resp.status().is_success() {
    // ì±„ë„ ìƒì„± (ë¹„ë™ê¸° ë©”ì‹œì§€ ì „ë‹¬)
    let (tx_event, rx_event) = mpsc::channel::<Result<ResponseEvent>>(1600);

    // ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    let stream = resp.bytes_stream().map_err(|e| {
        CodexErr::ResponseStreamFailed(ResponseStreamFailed {
            source: e,
            request_id: None,
        })
    });

    // ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ì—ì„œ SSE ì²˜ë¦¬
    tokio::spawn(process_chat_sse(
        stream,
        tx_event,
        provider.stream_idle_timeout(),
        otel_event_manager.clone(),
    ));

    // ìŠ¤íŠ¸ë¦¼ ë°˜í™˜ (ë¹„ë™ê¸° ì±„ë„ ìˆ˜ì‹  ì¸¡)
    return Ok(ResponseStream { rx_event });
}
```

**ì•„í‚¤í…ì²˜**:
```
HTTP Response
     â†“
Bytes Stream â”€â”€â†’ [Background Task: process_chat_sse]
                          â†“
                   Parse SSE Events
                          â†“
                   Parse JSON Chunks
                          â†“
                   mpsc::channel (tx_event)
                          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
ResponseStream (rx_event) â”€â”€â†’ Main Application
```

**í¬ì¸íŠ¸**:
- ë³„ë„ Tokio íƒœìŠ¤í¬ì—ì„œ SSE íŒŒì‹±
- `mpsc::channel`ë¡œ ë©”ì¸ ìŠ¤ë ˆë“œì™€ í†µì‹ 
- ë²„í¼ í¬ê¸° 1600ìœ¼ë¡œ ë°±í”„ë ˆì…” ê´€ë¦¬

### SSE íŒŒì‹± (ì‹¤ì œ êµ¬í˜„)

**ìœ„ì¹˜**: `codex-rs/core/src/chat_completions.rs:430+` (process_chat_sse í•¨ìˆ˜)

```rust
async fn process_chat_sse(
    stream: impl Stream<Item = Result<Bytes>>,
    tx_event: mpsc::Sender<Result<ResponseEvent>>,
    idle_timeout: Duration,
    otel_manager: OtelEventManager,
) {
    let mut event_stream = stream.eventsource();

    loop {
        // íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ìŠ¤íŠ¸ë¦¼ ì½ê¸°
        let event = match timeout(idle_timeout, event_stream.next()).await {
            Ok(Some(Ok(event))) => event,
            Ok(Some(Err(e))) => {
                // ìŠ¤íŠ¸ë¦¼ ì—ëŸ¬
                let _ = tx_event.send(Err(parse_error(e))).await;
                break;
            }
            Ok(None) | Err(_) => {
                // ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ
                break;
            }
        };

        // "[DONE]" ì²´í¬
        if event.data == "[DONE]" {
            let _ = tx_event.send(Ok(ResponseEvent::Done)).await;
            break;
        }

        // JSON íŒŒì‹±
        let chunk: ChatCompletionChunk = match serde_json::from_str(&event.data) {
            Ok(c) => c,
            Err(e) => {
                let _ = tx_event.send(Err(CodexErr::ParseError(e))).await;
                continue;
            }
        };

        // ë¸íƒ€ ì²˜ë¦¬
        if let Some(delta) = chunk.choices[0].delta.content {
            // í…ìŠ¤íŠ¸ ë¸íƒ€ë¥¼ ì´ë²¤íŠ¸ë¡œ ì „ì†¡
            let _ = tx_event.send(Ok(ResponseEvent::TextDelta(delta))).await;
        }
    }
}
```

**í¬ì¸íŠ¸**:
- `eventsource-stream` crateë¡œ SSE íŒŒì‹±
- Idle timeoutìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
- ê° ì²­í¬ë¥¼ `ResponseEvent`ë¡œ ë³€í™˜

### Idle Timeout ì²˜ë¦¬

**ìœ„ì¹˜**: `codex-rs/core/src/model_provider_info.rs` (ì¶”ì •)

```rust
impl ModelProviderInfo {
    pub fn stream_idle_timeout(&self) -> Duration {
        // ìŠ¤íŠ¸ë¦¼ì´ 60ì´ˆ ë™ì•ˆ ë°ì´í„° ì—†ìœ¼ë©´ íƒ€ì„ì•„ì›ƒ
        Duration::from_secs(60)
    }
}
```

**ë™ì‘**:
```rust
let event = timeout(idle_timeout, event_stream.next()).await;
```

- 60ì´ˆ ë™ì•ˆ ì²­í¬ê°€ ì˜¤ì§€ ì•Šìœ¼ë©´ ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨
- ëŠë¦° ìƒì„±ì´ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ê°ì§€

### ì¬ì‹œë„ì™€ ìŠ¤íŠ¸ë¦¬ë°

**ìœ„ì¹˜**: `codex-rs/core/src/chat_completions.rs:390-430`

```rust
// 429 (Rate Limit) ë˜ëŠ” 5xx ì—ëŸ¬ ì‹œ
Ok(res) if status == StatusCode::TOO_MANY_REQUESTS
         || status.is_server_error() => {

    if attempt > max_retries {
        return Err(CodexErr::RetryLimit(...));
    }

    // Retry-After í—¤ë” í™•ì¸
    let retry_after_secs = res
        .headers()
        .get(reqwest::header::RETRY_AFTER)
        .and_then(|v| v.to_str().ok())
        .and_then(|s| s.parse::<u64>().ok());

    // Retry-After ë˜ëŠ” exponential backoff
    let delay = retry_after_secs
        .map(|s| Duration::from_millis(s * 1_000))
        .unwrap_or_else(|| backoff(attempt));

    tokio::time::sleep(delay).await;
    // ì¬ì‹œë„...
}
```

**í¬ì¸íŠ¸**:
- ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì „ ì—ëŸ¬ëŠ” ì¬ì‹œë„ ê°€ëŠ¥
- ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ í›„ ì—ëŸ¬ëŠ” ì¬ì‹œë„ ë¶ˆê°€ (ë³„ë„ ì²˜ë¦¬)
- `Retry-After` í—¤ë” ìš°ì„  ì‚¬ìš©

### ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì „ë‹¬

```rust
// TUIì—ì„œ ìŠ¤íŠ¸ë¦¼ ì†Œë¹„
let mut stream = codex.send_message(user_input).await?;

while let Some(event) = stream.next().await {
    match event? {
        ResponseEvent::TextDelta(text) => {
            // í™”ë©´ì— ì¦‰ì‹œ ì¶œë ¥
            print!("{}", text);
            stdout().flush()?;
        }
        ResponseEvent::Done => {
            println!("\n[ì™„ë£Œ]");
            break;
        }
        _ => {}
    }
}
```

**ì‚¬ìš©ì ê²½í—˜**:
- ChatGPTì²˜ëŸ¼ íƒ€ì´í•‘ íš¨ê³¼
- ì¦‰ê°ì ì¸ í”¼ë“œë°±
- ì·¨ì†Œ ê°€ëŠ¥ (Ctrl+C)

**í•™ìŠµ í¬ì¸íŠ¸**:
- âœ… **ìŠ¤íŠ¸ë¦¬ë° ìš°ì„  ì„¤ê³„**: ëª¨ë“  ìš”ì²­ì´ ìŠ¤íŠ¸ë¦¬ë°
- âœ… **ë¹„ë™ê¸° ì•„í‚¤í…ì²˜**: Tokio + mpsc ì±„ë„
- âœ… **íƒ€ì„ì•„ì›ƒ ê´€ë¦¬**: Idle timeoutìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
- âœ… **ì¬ì‹œë„ ë¡œì§**: ìŠ¤íŠ¸ë¦¼ ì‹œì‘ ì „ ì—ëŸ¬ë§Œ ì¬ì‹œë„
- âœ… **Retry-After ì§€ì›**: ì„œë²„ ì§€ì‹œ ì¡´ì¤‘
- âœ… **ì—ëŸ¬ ì²˜ë¦¬**: íŒŒì‹± ì—ëŸ¬, ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë“± ëª¨ë‘ ì²˜ë¦¬

## ë‹¤ìŒ ë‹¨ê³„

ìŠ¤íŠ¸ë¦¬ë°ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ **ë³´ì´ëŠ” ê²ƒê³¼ ìˆ¨ê²¨ì§„ ê²ƒ**ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”íƒ€ë°ì´í„° ë“± ì¸í„°í˜ì´ìŠ¤ì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ë¥¼ íŒŒí—¤ì³ë´…ì‹œë‹¤.

â¡ï¸ [7ì¥: ë³´ì´ëŠ” ê²ƒê³¼ ìˆ¨ê²¨ì§„ ê²ƒ](./07-visible-vs-hidden.md)

---

## í•µì‹¬ ìš”ì•½

- âœ… ìŠ¤íŠ¸ë¦¬ë°ì€ `stream=True`ë¡œ í™œì„±í™”
- âœ… ì‘ë‹µì´ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì „ë‹¬ë¨
- âœ… `delta.content`ì— í…ìŠ¤íŠ¸ ì¡°ê° í¬í•¨
- âœ… `finish_reason`ìœ¼ë¡œ ì™„ë£Œ ìƒíƒœ í™•ì¸
- âœ… Server-Sent Events (SSE) í”„ë¡œí† ì½œ ì‚¬ìš©
- âœ… í•¨ìˆ˜ í˜¸ì¶œë„ ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥
- âœ… ì›¹ì•± í†µí•© ì‹œ EventSource ë˜ëŠ” Fetch API ì‚¬ìš©
- âœ… íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„, ì·¨ì†Œ ë¡œì§ êµ¬í˜„ ê¶Œì¥
