# 6ì¥: ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤

ìŠ¤íŠ¸ë¦¬ë°(Streaming)ì€ LLMì˜ ì‘ë‹µì„ **ì „ì²´ê°€ ì™„ì„±ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ê¸ˆì”© ë°›ì•„ì˜¤ëŠ”** ë°©ì‹ì…ë‹ˆë‹¤. ChatGPT ì›¹ ì¸í„°í˜ì´ìŠ¤ì²˜ëŸ¼ í…ìŠ¤íŠ¸ê°€ íƒ€ì´í•‘ë˜ë“¯ ë‚˜íƒ€ë‚˜ëŠ” íš¨ê³¼ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì™œ ìŠ¤íŠ¸ë¦¬ë°ì´ í•„ìš”í•œê°€?

### ì¼ê´„ ì‘ë‹µ (Non-streaming)

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}]
)

# â³ 10ì´ˆ ëŒ€ê¸°...

print(response.choices[0].message.content)
# ì „ì²´ ì‘ë‹µì´ í•œ ë²ˆì— ì¶œë ¥
```

**ë¬¸ì œì **:
- âŒ ì‚¬ìš©ìê°€ ì˜¤ë«ë™ì•ˆ ê¸°ë‹¤ë ¤ì•¼ í•¨
- âŒ ì‘ë‹µì´ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì„
- âŒ UX ì €í•˜

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

# "ì˜›" "ë‚ " " " "ì˜›" "ì " "ì—" " " ...
# ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¨ì–´ê°€ ì¶œë ¥ë¨!
```

**ì¥ì **:
- âœ… ì¦‰ê°ì ì¸ í”¼ë“œë°±
- âœ… ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜
- âœ… ê¸´ ì‘ë‹µì—ë„ ë°˜ì‘ì„± ìœ ì§€
- âœ… ì¤‘ê°„ì— ì·¨ì†Œ ê°€ëŠ¥

## OpenAI ìŠ¤íŠ¸ë¦¬ë°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Pythonì´ ë­ì•¼?"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

### ì²­í¬(Chunk) êµ¬ì¡°

ê° ì²­í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

```python
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion.chunk",  # "chunk"ì„ì— ì£¼ëª©
  "created": 1699564800,
  "model": "gpt-4-0613",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",     # ì²« ì²­í¬ì—ë§Œ
        "content": "Pythonì€"     # ì‹¤ì œ í…ìŠ¤íŠ¸ ì¡°ê°
      },
      "finish_reason": null      # ë§ˆì§€ë§‰ ì²­í¬ì—ì„œë§Œ "stop" ë“±
    }
  ]
}
```

**ì²­í¬ ì‹œí€€ìŠ¤ ì˜ˆì‹œ**:

```python
# Chunk 1
{"delta": {"role": "assistant", "content": ""}, "finish_reason": null}

# Chunk 2
{"delta": {"content": "Python"}, "finish_reason": null}

# Chunk 3
{"delta": {"content": "ì€"}, "finish_reason": null}

# Chunk 4
{"delta": {"content": " í”„ë¡œê·¸ë˜ë°"}, "finish_reason": null}

# ...

# ë§ˆì§€ë§‰ Chunk
{"delta": {}, "finish_reason": "stop"}
```

### ì „ì²´ ì‘ë‹µ ì¬êµ¬ì„±

```python
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ì•ˆë…•"}],
    stream=True
)

full_response = ""
for chunk in stream:
    if chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
        full_response += content
        print(content, end="", flush=True)

print(f"\n\nì „ì²´ ì‘ë‹µ: {full_response}")
```

### finish_reason ì²˜ë¦¬

```python
for chunk in stream:
    choice = chunk.choices[0]

    if choice.delta.content:
        print(choice.delta.content, end="", flush=True)

    if choice.finish_reason == "stop":
        print("\nâœ“ ì •ìƒ ì™„ë£Œ")
    elif choice.finish_reason == "length":
        print("\nâš  max_tokens ë„ë‹¬")
    elif choice.finish_reason == "function_call":
        print("\nğŸ”§ í•¨ìˆ˜ í˜¸ì¶œ ìš”ì²­")
    elif choice.finish_reason == "content_filter":
        print("\nğŸš« ì»¨í…ì¸  í•„í„°ë§")
```

## Function Callingê³¼ ìŠ¤íŠ¸ë¦¬ë°

ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œë„ í•¨ìˆ˜ í˜¸ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "ë‚ ì”¨ ì •ë³´ ì¡°íšŒ",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }
        }
    }
]

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ì„œìš¸ ë‚ ì”¨ëŠ”?"}],
    tools=tools,
    stream=True
)

function_name = ""
function_args = ""

for chunk in stream:
    delta = chunk.choices[0].delta

    # í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ê°€ ì¡°ê°ë‚˜ì„œ ì˜´
    if delta.tool_calls:
        for tool_call in delta.tool_calls:
            if tool_call.function.name:
                function_name += tool_call.function.name
            if tool_call.function.arguments:
                function_args += tool_call.function.arguments

    if chunk.choices[0].finish_reason == "tool_calls":
        print(f"í•¨ìˆ˜ í˜¸ì¶œ: {function_name}({function_args})")
        # í•¨ìˆ˜ ì‹¤í–‰ ë¡œì§...
```

## Server-Sent Events (SSE)

ìŠ¤íŠ¸ë¦¬ë°ì€ ë‚´ë¶€ì ìœ¼ë¡œ **Server-Sent Events** í”„ë¡œí† ì½œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### SSE í˜•ì‹

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{"content":" world"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","choices":[{"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### ì§ì ‘ SSE ì²˜ë¦¬ (ê³ ê¸‰)

```python
import requests
import json

response = requests.post(
    "https://api.openai.com/v1/chat/completions",
    headers={
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    },
    json={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": "Hello"}],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith("data: "):
            data = line[6:]  # "data: " ì œê±°
            if data == "[DONE]":
                break
            chunk = json.loads(data)
            if chunk["choices"][0]["delta"].get("content"):
                print(chunk["choices"][0]["delta"]["content"], end="")
```

## Anthropic ìŠ¤íŠ¸ë¦¬ë°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import anthropic

client = anthropic.Anthropic()

with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### ì´ë²¤íŠ¸ ê¸°ë°˜ ì²˜ë¦¬

```python
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
) as stream:
    for event in stream:
        if event.type == "content_block_start":
            print("ì‘ë‹µ ì‹œì‘")
        elif event.type == "content_block_delta":
            print(event.delta.text, end="")
        elif event.type == "content_block_stop":
            print("\nì‘ë‹µ ì™„ë£Œ")
        elif event.type == "message_stop":
            print(f"í† í° ì‚¬ìš©: {stream.message.usage}")
```

## ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•©

### FastAPI + SSE

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI
import json

app = FastAPI()
client = OpenAI()

@app.get("/chat")
async def chat_stream(message: str):
    def generate():
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": message}],
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                # SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                data = {
                    "content": chunk.choices[0].delta.content,
                    "done": False
                }
                yield f"data: {json.dumps(data)}\n\n"

        # ì™„ë£Œ ì‹ í˜¸
        yield f"data: {json.dumps({'done': True})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### JavaScript í´ë¼ì´ì–¸íŠ¸

```javascript
// EventSource API ì‚¬ìš©
const eventSource = new EventSource('/chat?message=ì•ˆë…•í•˜ì„¸ìš”');

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.done) {
        eventSource.close();
        console.log('ì‘ë‹µ ì™„ë£Œ');
    } else {
        // í™”ë©´ì— í…ìŠ¤íŠ¸ ì¶”ê°€
        document.getElementById('response').textContent += data.content;
    }
};

eventSource.onerror = (error) => {
    console.error('SSE ì—ëŸ¬:', error);
    eventSource.close();
};
```

### Fetch API + Streaming

```javascript
async function streamChat(message) {
    const response = await fetch('/chat', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({message})
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const {done, value} = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = JSON.parse(line.slice(6));
                if (data.content) {
                    document.getElementById('response').textContent += data.content;
                }
            }
        }
    }
}
```

## React í†µí•© ì˜ˆì œ

```typescript
import { useState } from 'react';
import OpenAI from 'openai';

export function ChatComponent() {
    const [response, setResponse] = useState('');
    const [isStreaming, setIsStreaming] = useState(false);

    const handleSubmit = async (message: string) => {
        setIsStreaming(true);
        setResponse('');

        const client = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY,
            dangerouslyAllowBrowser: true  // í”„ë¡œë•ì…˜ì—ì„œëŠ” ë°±ì—”ë“œ ì‚¬ìš©!
        });

        const stream = await client.chat.completions.create({
            model: 'gpt-4',
            messages: [{role: 'user', content: message}],
            stream: true
        });

        for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            setResponse(prev => prev + content);
        }

        setIsStreaming(false);
    };

    return (
        <div>
            <div className="response">
                {response}
                {isStreaming && <span className="cursor">â–‹</span>}
            </div>
            <button onClick={() => handleSubmit('ì•ˆë…•í•˜ì„¸ìš”')}>
                ì „ì†¡
            </button>
        </div>
    );
}
```

## ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì—°ê²°

### Timeout ì²˜ë¦¬

```python
import signal

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException()

# íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ)
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(30)

try:
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "ê¸´ ì‘ë‹µ ìš”ì²­"}],
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="")

    signal.alarm(0)  # íƒ€ì„ì•„ì›ƒ í•´ì œ

except TimeoutException:
    print("\nâ± ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
```

### ì¬ì‹œë„ ë¡œì§

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def stream_with_retry(messages):
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        stream=True
    )

    response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            response += content
            yield content

    return response

# ì‚¬ìš©
try:
    for content in stream_with_retry([{"role": "user", "content": "ì•ˆë…•"}]):
        print(content, end="")
except Exception as e:
    print(f"ìµœì¢… ì‹¤íŒ¨: {e}")
```

## ì„±ëŠ¥ ìµœì í™”

### ë²„í¼ë§

```python
import time

class StreamBuffer:
    def __init__(self, flush_interval=0.1):
        self.buffer = []
        self.flush_interval = flush_interval
        self.last_flush = time.time()

    def add(self, text):
        self.buffer.append(text)

        # ì¼ì • ì‹œê°„ë§ˆë‹¤ ë˜ëŠ” ë²„í¼ê°€ ì°¨ë©´ í”ŒëŸ¬ì‹œ
        if time.time() - self.last_flush > self.flush_interval or len(self.buffer) > 10:
            self.flush()

    def flush(self):
        if self.buffer:
            print(''.join(self.buffer), end='', flush=True)
            self.buffer = []
            self.last_flush = time.time()

# ì‚¬ìš©
buffer = StreamBuffer()

stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ê¸´ ì‘ë‹µ"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        buffer.add(chunk.choices[0].delta.content)

buffer.flush()  # ë§ˆì§€ë§‰ ë²„í¼ ë¹„ìš°ê¸°
```

## ì¤‘ê°„ ì·¨ì†Œ

```python
import threading

class CancellableStream:
    def __init__(self):
        self.cancelled = False

    def cancel(self):
        self.cancelled = True

    def stream(self, messages):
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            stream=True
        )

        for chunk in stream:
            if self.cancelled:
                print("\nâ¹ ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œë¨")
                break

            if chunk.choices[0].delta.content:
                print(chunk.choices[0].delta.content, end="")

# ì‚¬ìš©
cs = CancellableStream()

# ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë°
thread = threading.Thread(
    target=cs.stream,
    args=([{"role": "user", "content": "ë§¤ìš° ê¸´ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”"}],)
)
thread.start()

# ì‚¬ìš©ìê°€ ì·¨ì†Œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´
# cs.cancel()
```

## ë‹¤ìŒ ë‹¨ê³„

ìŠ¤íŠ¸ë¦¬ë°ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ì´ì œ **ë³´ì´ëŠ” ê²ƒê³¼ ìˆ¨ê²¨ì§„ ê²ƒ**ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”íƒ€ë°ì´í„° ë“± ì¸í„°í˜ì´ìŠ¤ì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ë¥¼ íŒŒí—¤ì³ë´…ì‹œë‹¤.

â¡ï¸ [7ì¥: ë³´ì´ëŠ” ê²ƒê³¼ ìˆ¨ê²¨ì§„ ê²ƒ](./07-visible-vs-hidden.md)

---

## í•µì‹¬ ìš”ì•½

- âœ… ìŠ¤íŠ¸ë¦¬ë°ì€ `stream=True`ë¡œ í™œì„±í™”
- âœ… ì‘ë‹µì´ ì²­í¬(chunk) ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì „ë‹¬ë¨
- âœ… `delta.content`ì— í…ìŠ¤íŠ¸ ì¡°ê° í¬í•¨
- âœ… `finish_reason`ìœ¼ë¡œ ì™„ë£Œ ìƒíƒœ í™•ì¸
- âœ… Server-Sent Events (SSE) í”„ë¡œí† ì½œ ì‚¬ìš©
- âœ… í•¨ìˆ˜ í˜¸ì¶œë„ ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥
- âœ… ì›¹ì•± í†µí•© ì‹œ EventSource ë˜ëŠ” Fetch API ì‚¬ìš©
- âœ… íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„, ì·¨ì†Œ ë¡œì§ êµ¬í˜„ ê¶Œì¥
